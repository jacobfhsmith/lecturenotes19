---
title: "Linear Regression III: Multiple Regression"
author: ""
date: "March 29, 2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

## Coming up
- Exam 2 released on Thursday afternoon, due next Monday at 11:59 PM
- No lab this week.
- Stat experience day next Monday.

## Main ideas

- Review and expand upon concepts from our first two regression classes.
- Learn how to carry out and interpret multiple linear regressions.
- Learn how to assess the conditions for inference in regression. 

## Packages

We'll use the `tidyverse`, `broom`, and `viridis` packages again, as well as the `car` package when calculate variance inflation factors (VIFs) to examine whether our models have multicollinearity.

```{r packages, include=FALSE}
library(tidyverse)
library(broom)
library(viridis)
library(car)
```

## Please recall

- Response Variable: Variable whose behavior or variation you are trying to understand, on the y-axis. Also called the dependent variable.

- Explanatory Variable: Other variables that you want to use to explain the variation in the response, on the x-axis. Also called independent variables, predictors, or features.

- Predicted value: Output of the model function
  - The model function gives the typical value of the response variable   conditioning on the explanatory variables (what does this mean?)

- Residuals: Shows how far each case is from its predicted value
  - Residual = Observed value - Predicted value
  - Tells how far above/below the model function each case is
  
## The linear model with a single predictor

- We're interested in the $\beta_0$ (population parameter for the intercept)
and the $\beta_1$ (population parameter for the slope) in the 
following model:

$$ \hat{y} = \beta_0 + \beta_1~x + \epsilon $$

- Unfortunately, we can't get these values

- So we use sample statistics to estimate them:

$$ \hat{y} = b_0 + b_1~x $$

## Least squares regression

The regression line minimizes the sum of squared residuals.

- **Residuals**: $e_i = y_i - \hat{y}_i$,

- The regression line minimizes $\sum_{i = 1}^n e_i^2$.

- Equivalently, minimizing $\sum_{i = 1}^n [y_i - (b_0 + b_1~x_i)]^2$

## Data

```{r loaddata, message=FALSE}
sports_car_prices <- read.csv("~/sportscars.csv")
```

The file `sportscars.csv` contains prices for Porsche and Jaguar cars for sale
on cars.com.

`car`: car make (Jaguar or Porsche)

`price`: price in USD

`age`: age of the car in years

`mileage`: previous miles driven

## The linear model with a single predictor

```{r pricesmodel}
prices_model<-(lm(price ~ age, data = sports_car_prices))
tidy(prices_model)
```

But is the age the only variable that predicts price?

## The linear model with multiple predictors

$$ \hat{y} = \beta_0 + \beta_1~x_1 + \beta_2~x_2 + \cdots + \beta_k~x_k +\epsilon $$

- Sample model that we use to estimate the population model:
  
$$ \hat{y} = b_0 + b_1~x_1 + b_2~x_2 + \cdots + b_k~x_k $$

Let's add a variable. 

## Price vs. age and type of car

Does the relationship between price and age depend on type of car?

```{r plot1, echo=FALSE, warning=FALSE}
ggplot(data = sports_car_prices, 
       aes(x = age, y = price, color = car)) + 
  scale_color_viridis(discrete = TRUE, option = "D", name = "Type of Car") + 
  labs(x = "Age (years)", y = "Price (USD)", color = "Car Make") + 
  geom_point() + 
  geom_smooth(method="lm", se = FALSE) 
```

## Modeling with main effects 

```{r maineffects}
m_main <- lm(price ~ age + car, data = sports_car_prices)
m_main %>%
  tidy() %>%
  select(term, estimate)
```

Linear model:

$$ \widehat{price} = 44310 - 2487~age + 21648~carPorsche $$


- Plug in 0 for `carPorsche` to get the linear model for Jaguars.
- Plug in 1 for `carPorsche` to get the linear model for Porsches.

- Jaguar: 
$$\begin{align}\widehat{price} &= 44310 - 2487~age + 21648 \times 0\\
&= 44310 - 2487~age\\\end{align}$$

- Porsche: 
$$\begin{align}\widehat{price} &= 44310 - 2487~age + 21648 \times 1\\
&= 65958 - 2487~age\\\end{align}$$


- Rate of change in price as the age of the car increases does not depend on 
make of car (same slopes)
- Porsches are consistently more expensive than Jaguars (different intercepts)

## Interpretation of main effects

```{r plot2, fig.height=4, echo = FALSE}

ggplot(data = sports_car_prices, 
       aes(x = age, y = price, color = car)) + 
  scale_color_viridis(discrete = TRUE, option = "D", name = "Type of Car") +
  geom_point()  +
  geom_abline(intercept = 44310, slope = -2487, color = "#5B2C6F", lwd = 1) +
  geom_abline(intercept = 65958, slope = -2487, color = "#F7DC6F", lwd = 1) +
  labs(x = "Age (years)", y = "Price (USD)", color = "Car Make")
```

## Main effects, numerical and categorical predictors

```{r maineffects2}
m_main %>%
  tidy() %>%
  select(term, estimate)
```

```{r maincoefs}
m_main_coefs <- m_main %>%
  tidy() %>%
  select(term, estimate)
m_main_coefs
```

- **All else held constant**, for each additional year of a car's age, the price
of the car is predicted to decrease, on average, by $2,487.

- **All else held constant**, Porsches are predicted, on average, to have a 
price that is $21,648 greater than Jaguars.

- Jaguars that have an age of 0 are predicted, on average, to have a price of $44,310.

## What went wrong?

**Question**:
Why is our linear regression model different from what we got from `geom_smooth(method = "lm")`?

- `car` is the only variable in our model that affects the intercept.

- The model we specified assumes Jaguars and Porsches have the **same slope** 
and **different intercepts**.

- What is the most appropriate model for these data?

  - same slope and intercept for Jaguars and Porsches?
  - same slope and different intercept for Jaguars and Porsches?
  - different slope and different intercept for Jaguars and Porsches?
  
## Interacting explanatory variables

- Including an interaction effect in the model allows for different slopes, i.e. 
nonparallel lines.

- This means that the relationship between an explanatory variable and the 
response depends on another explanatory variable.

- We can accomplish this by adding an **interaction variable**. This is the 
product of two explanatory variables (also sometimes called an interaction term).


## Modeling with interaction effects

```{r interactingplot}
ggplot(data = sports_car_prices, 
       aes(x = age, y = price, color = car)) + 
    scale_color_viridis(discrete = TRUE, option = "D", name = "Type of Car") + 
  labs(x = "Age (years)", y = "Price (USD)", color = "Car Make") + 
  geom_point() + 
  geom_smooth(method="lm", se = FALSE) 
``` 

```{r interaction}
 m_int <- lm(price ~ age + car + age * car, 
            data = sports_car_prices) 
m_int %>%
  tidy() %>%
  select(term, estimate)
```

$$\widehat{price} = 56988 - 5040~age + 6387~carPorsche + 2969~age \times carPorsche$$

## Interpretation of interaction effects

$$\widehat{price} = 56988 - 5040~age + 6387~carPorsche + 2969~age \times carPorsche$$

- Plug in 0 for `carPorsche` to get the linear model for Jaguars.
- Plug in 1 for `carPorsche` to get the linear model for Porsches.

- Jaguar: 

$$\begin{align}\widehat{price} &= 56988 - 5040~age + 6387~carPorsche + 2969~age \times carPorsche \\
&= 56988 - 5040~age + 6387 \times 0 + 2969~age \times 0\\
&= 56988 - 5040~age\end{align}$$

- Porsche:


$$\begin{align}\widehat{price} &= 56988 - 5040~age + 6387~carPorsche + 2969~age \times carPorsche \\
&= 56988 - 5040~age + 6387 \times 1 + 2969~age \times 1\\
&= 63375 - 2071~age\end{align}$$

## Interpretation of interaction effects

- Jaguar: 

$$\widehat{price} = 56988 - 5040~age$$

- Porsche: 

$$\widehat{price} = 63375 - 2071~age$$

- Rate of change in price as the age of the car increases depends on the make 
of the car (different slopes).

- Porsches are consistently more expensive than Jaguars (different intercepts).

## Continuous by continuous interactions

- Interpretation becomes trickier

- Slopes conditional on values of explanatory variables

## Third order interactions

- Can you? Yes

- Should you? Probably not if you want to interpret these interactions in 
context of the data.

## Assessing quality of model fit: Adjusted $R^2$

## To Review:
- The strength of the fit of a linear model is commonly evaluated using $R^2$.

- It tells us what percentage of the variability in the response variable is explained by the model. The remainder of the variability is unexplained.

- $R^2$ is sometimes called the coefficient of determination.

## Obtaining $R^2$ in R

Let's obtain $R^2$ for our simple model with just age as an explanatory variable.

```{r rsq}
glance(prices_model) %>%
  pull(r.squared)
```

Roughly 27% of the variability in price of used cars can be explained by age. Notice here that we have two variables in this model. Last class, we worked with $R^2$ with a single explanatory variable. Now, let's look at the R-squared for our other two models.

```{r rsqothermodels}
glance(m_main) %>% 
  pull(r.squared)  
glance(m_int) %>% 
  pull(r.squared) 
```

- The model with both age and make has an $R^2$ of about 61% and the model with the interaction term has an even higher $R^2$.

- Using $R^2$ for model selection in models with multiple explanatory 
variables is not a good idea as $R^2$ increases when **any** variable is added to the model.

## Please recall:

- We can write explained variation using the following ratio of sums of squares:

$$ R^2 =  1 - \left( \frac{ SS\_{Error} }{ SS\_{Total} } \right) $$

where $SS_{Error}$ is the sum of squared residuals and $SS_{Total}$ is the total
variance in the response variable.

## Adjusted $R^2$

$$ R^2\_{adj} = 1 - \left( \frac{ SS\_{Error} }{ SS\_{Total} } \times \frac{n - 1}{n - k - 1} \right), $$

where $n$ is the number of observations and $k$ is the number of predictors in 
the model.

- Adjusted $R^2$ doesn't increase if the new variable does not provide any new 
information or is completely unrelated and can even decrease.

- This makes adjusted $R^2$ a preferable metric for model selection in multiple
regression models.

Let's get the adjusted $R^2$

```{r rsqothermodels2}
glance(m_main) %>% 
  pull(adj.r.squared)  
glance(m_int) %>% 
  pull(adj.r.squared) 
```

## Regression Diagnostics & Conditions for Inference in Regression

## Conditions

- Linearity: The relationship between response and predictor(s) is linear
- Independence: The residuals are independent
- Normality: The residuals are nearly normally distributed
- Equal Variance: The residuals have constant variance

## Or, specifically

- **L**inearity: The relationship between response and predictor(s) is linear
- **I**ndependence: The residuals are independent
- **N**ormality: The residuals are nearly normally distributed
- **E**qual Variance: The residuals have constant variance

- For multiple regression, the predictors shouldn't be too correlated with each 
other. 

## Tidy model output

```{r output}
tidy(m_int) %>%
  select(term, estimate) %>%
  mutate(estimate = round(estimate, 3))
```

## `augment` data with model results

- `.fitted`: Predicted value of the response variable
- `.resid`: Residuals

```{r augment}
m_int_aug <- augment(m_int)
m_int_aug %>%
  slice(1:3)
```

We will use the fitted values and residuals to check the conditions by 
constructing **diagnostic plots**.

## Price vs age to examine linearity

```{r simpleplot}
ggplot(data = sports_car_prices, 
       aes(x = age, 
           y = price)) + 
  geom_point()
```

## Residuals in order of collection

### Independence

```{r independence}
ggplot(data = m_int_aug, 
       aes(x = 1:nrow(sports_car_prices), 
           y = .resid)) + 
  geom_point() + 
  labs(x = "Index", y = "Residual")
```

## Residuals vs fitted plot

### Equal Variance, Linearity

```{r eqvarlinear}
ggplot(m_int_aug, mapping = aes(x = .fitted, y = .resid)) +
  geom_point() + 
  geom_hline(yintercept = 0, lwd = 2, col = "red", lty = 2) +
  labs(x = "Predicted Price", y = "Residuals") 
```

## Histogram of residuals

### Normality

```{r normal}
ggplot(m_int_aug, mapping = aes(x = .resid)) +
  geom_histogram(bins = 15) + 
  labs(x = "Residuals")
```

## Normal Q-Q Plot

### Normality

```{r qq}
ggplot(m_int_aug, mapping = aes(sample = .resid)) +
  stat_qq() + 
  stat_qq_line()
```

## Multicollinearity

You don't want the predictors to be too correlated with each 
other in a multiple regression model. When they are correlated with each other, you have **mutlicollinearity**. One way to diagnose multicollinearity is with **variance inflation factors.** There's no specific cutoff, but a VIF of 10 if sometimes used as a cutoff.

Let's see if we have multicollinearity in our first model.
```{r vif}
tibble(vif(m_main))
```

Now, let's check if for the interactive model.

```{r vif2}
tibble(vif(m_int))
```

Notice the VIFs here are higher. This is to be expected with an interactive model. 

**Question**: Why do you think VIFs will be higher in interactive models?

## Practice

1. Run and interpret a multiple regression with both age and mileage as predictors. Are both of these statistically significant predictors of the price of a car?

```{r agemileage}

```

2. Find and interpret the adjusted $R^2$ for this model.

```{r adjr2mileage}

```

3. Examine the extent to which there is multicollinearity in this model.

```{r multi}
```

Now, please turn to the dataset in `nycairquality.csv`. This file contains daily air quality measurements in New York from May to September 1973 and collected by the New York State Department of Conservation and the National Weather Service (Chambers, J. M., Cleveland, W. S., Kleiner, B. and Tukey, P. A. (1983) *Graphical Methods for Data Analysis*. Belmont, CA: Wadsworth).

- `Ozone`: ozone (ppb)
- `Solar.R`: solar radiation (langleys)
- `Wind`: wind (mpg)
- `Temp`: temperature (degrees F)

```{r loaddata2, message=FALSE}
airquality <- read.csv("~/airquality.csv")
```

4. Please run and interpret a model with ozone in parts per billion as the response variable and solar radiation, wind, and temperature as the explanatory variables.

```{r ozonemodel}

```

5. Use augment to obtain the value of residuals and assess independence with a plot.

```{r independenceplot}

```

6. Next, make a plot comparing predicted values to residuals to assess the equal variance and linearity conditions.

```{r predictedvaluesplot}

```

7. Finally, make a histogram and a q-q plot to assess the normality condition.

```{r histogramair}

```

```{r qqair}

```

## For Next Class
- Please read OIS sections 9.2 and 9.3.